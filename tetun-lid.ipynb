{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from text_processing.process_data import *\n",
    "from training.train_dev_test import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dili, 11 Outubru 2021 - Gabinete Apoiu Ativida...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Treinamentu ne’e ninia objetivu prinsipál mak ...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iha loron daruak hosi treinamentu ne’e partisi...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Partisipante na’in-56 iha treinamentu ne’e mai...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Komunidade iha suku Bikeli ho Makadade agrades...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence language\n",
       "0  Dili, 11 Outubru 2021 - Gabinete Apoiu Ativida...      tet\n",
       "1  Treinamentu ne’e ninia objetivu prinsipál mak ...      tet\n",
       "2  Iha loron daruak hosi treinamentu ne’e partisi...      tet\n",
       "3  Partisipante na’in-56 iha treinamentu ne’e mai...      tet\n",
       "4  Komunidade iha suku Bikeli ho Makadade agrades...      tet"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths\n",
    "files_folder = 'lang_files/'\n",
    "files_path = os.path.join(os.getcwd(), files_folder)\n",
    "\n",
    "# initial data\n",
    "lang_data = compile_all_data(files_path)\n",
    "lang_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dili  outubru gabinete apoiu atividade kónjuge...</td>\n",
       "      <td>tet</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>treinamentu ne’e ninia objetivu prinsipál mak ...</td>\n",
       "      <td>tet</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iha loron daruak hosi treinamentu ne’e partisi...</td>\n",
       "      <td>tet</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>partisipante na’in iha treinamentu ne’e mai ho...</td>\n",
       "      <td>tet</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>komunidade iha suku bikeli ho makadade agrades...</td>\n",
       "      <td>tet</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence language  sentence_length\n",
       "0  dili  outubru gabinete apoiu atividade kónjuge...      tet              275\n",
       "1  treinamentu ne’e ninia objetivu prinsipál mak ...      tet              232\n",
       "2  iha loron daruak hosi treinamentu ne’e partisi...      tet              288\n",
       "3  partisipante na’in iha treinamentu ne’e mai ho...      tet               82\n",
       "4  komunidade iha suku bikeli ho makadade agrades...      tet              320"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after preprocessed and counted each sentence length\n",
    "clean_data = clean_data_with_count(files_path)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is cleaned.\n"
     ]
    }
   ],
   "source": [
    "# confirm that data was cleaned.\n",
    "clean = clean_data[(clean_data['sentence'] =='') & (clean_data['sentence'] ==' ')]\n",
    "try:\n",
    "    assert len(clean) == 0\n",
    "    print(\"The data is cleaned.\")\n",
    "except AssertionError:\n",
    "    print(\"The data is NOT cleaned.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 74258\n",
      "Dev set size: 15913\n",
      "Test set size: 15913\n"
     ]
    }
   ],
   "source": [
    "# Split dataset to train, development(dev)/validation, and test sets\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = train_dev_test_split(clean_data, 0.3, 0.5)\n",
    "\n",
    "# print the sizes of the resulting sets\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Dev set size:\", len(X_dev))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinearSVC()\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.9798\n",
      "\tn_gram 2 --> accuracy:  0.9927\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.9938\n",
      "\tn_gram 2 --> accuracy:  0.9666\n",
      "Model: LogisticRegression(multi_class='ovr')\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.9789\n",
      "\tn_gram 2 --> accuracy:  0.9924\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.9913\n",
      "\tn_gram 2 --> accuracy:  0.9519\n",
      "Model: MultinomialNB()\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.9412\n",
      "\tn_gram 2 --> accuracy:  0.9911\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.9960\n",
      "\tn_gram 2 --> accuracy:  0.9708\n"
     ]
    }
   ],
   "source": [
    "# Compare the models\n",
    "\n",
    "model_lists = [LinearSVC(), LogisticRegression(multi_class='ovr'), MultinomialNB()]\n",
    "analyzers = ['char_wb', 'word']\n",
    "\n",
    "compare_models(model_lists, analyzers, 1, 6, 1, X_train, y_train, X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.997109281719349\n",
      "Confusion Matrix:  [[5050    2    3    2]\n",
      " [  14 4254    2    0]\n",
      " [  13    2 4020    1]\n",
      " [   5    0    2 2543]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "          en       0.99      1.00      1.00      5057\n",
      "          id       1.00      1.00      1.00      4270\n",
      "          pt       1.00      1.00      1.00      4036\n",
      "         tet       1.00      1.00      1.00      2550\n",
      "\n",
      "    accuracy                           1.00     15913\n",
      "   macro avg       1.00      1.00      1.00     15913\n",
      "weighted avg       1.00      1.00      1.00     15913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the best model, train and evaluate it using dev set\n",
    "\n",
    "# Train the model\n",
    "model = train_model(TfidfVectorizer(analyzer='char_wb',ngram_range=(5,5)), MultinomialNB(), X_train, y_train)\n",
    "\n",
    "# Evaluate the model using dev set\n",
    "evaluate_model(model,X_dev, y_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9967950732105826\n",
      "Confusion Matrix:  [[5001    3    3    3]\n",
      " [  15 4260    3    2]\n",
      " [   9    3 4008    3]\n",
      " [   3    0    4 2593]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "          en       0.99      1.00      1.00      5010\n",
      "          id       1.00      1.00      1.00      4280\n",
      "          pt       1.00      1.00      1.00      4023\n",
      "         tet       1.00      1.00      1.00      2600\n",
      "\n",
      "    accuracy                           1.00     15913\n",
      "   macro avg       1.00      1.00      1.00     15913\n",
      "weighted avg       1.00      1.00      1.00     15913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using test set\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 classification: ['tet']\n",
      "Test 2 classification: ['tet']\n"
     ]
    }
   ],
   "source": [
    "test1 = model.predict([\"Organizasaun mundial saúde\"])\n",
    "test2 = model.predict([\"Tribunál rekursu rejeita kandidatura partidu\"])\n",
    "print(f\"Test 1 classification: {test1}\\nTest 2 classification: {test2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deklarasaun Universál Direitus Umanus\n",
      "en 1.7198102347965515e-05\n",
      "id 7.656943219400005e-05\n",
      "pt 0.0001802486040840422\n",
      "tet 0.9997259838613715\n",
      "Indonesia merupakan negara terluas ke-14 sekaligus\n",
      "en 1.3050918693531165e-07\n",
      "id 0.9999995025902451\n",
      "pt 9.055449712082101e-08\n",
      "tet 2.7634607120579004e-07\n",
      "A língua portuguesa, também designada português, é uma língua\n",
      "en 0.00018657712215813118\n",
      "id 2.970099994212303e-05\n",
      "pt 0.998175916996359\n",
      "tet 0.0016078048815381433\n",
      "Deklarasaun ne'e inklui artigu 30 ne'ebé esplika Asembleia Jerál\n",
      "en 2.916091141335691e-08\n",
      "id 4.085156091273892e-08\n",
      "pt 1.4132462405706621e-07\n",
      "tet 0.9999997886629056\n",
      "Can we feed a future population of 10 billion people a healthy?\n",
      "en 0.9999978448630447\n",
      "id 1.321850425292166e-07\n",
      "pt 1.061105899088093e-06\n",
      "tet 9.61846016528177e-07\n"
     ]
    }
   ],
   "source": [
    "input = [\"Deklarasaun Universál Direitus Umanus\", \n",
    "        \"Indonesia merupakan negara terluas ke-14 sekaligus\",\n",
    "        \"A língua portuguesa, também designada português, é uma língua\",\n",
    "        \"Deklarasaun ne'e inklui artigu 30 ne'ebé esplika Asembleia Jerál\",\n",
    "        \"Can we feed a future population of 10 billion people a healthy?\"\n",
    "        ]\n",
    "\n",
    "# Naive Bayes and Logistic Regression\n",
    "pred_probs = model.predict_proba(input)\n",
    "\n",
    "for i, probs in enumerate(pred_probs):\n",
    "    print(input[i])\n",
    "    for j, lang in enumerate(model.classes_):\n",
    "        print(lang, probs[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - LinearSVC\n",
    "pred_result = model.predict(input)\n",
    "pred_probability = model.decision_function(input)\n",
    "for i in range(len(input)):\n",
    "    print(f\"{input[i]} ---> {pred_result[i]} ---> {np.argmax(pred_probability[i])} --> {pred_probability[i]} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model [if required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_best/tet-lid-model_NB_best_ng5chars.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# save the model to a file\n",
    "joblib.dump(model, 'model_best/tet-lid-model_NB_best_ng5chars.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the save model from a file\n",
    "saved_model = joblib.load('model_best/tet-lid-model_NB_best_ng5chars.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLa Partisipa Aniversariu Veteranu Ba Dala VI Xanana: \"Ha\\'u La Halo Buat Ida, Husu Ba Jeneral no Fundador Sira, Tanba Ha\\'u Ne\\'e Soeharto Kedua\" \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "text = \"\"\"\n",
    "𝐋𝐚 𝐏𝐚𝐫𝐭𝐢𝐬𝐢𝐩𝐚 𝐀𝐧𝐢𝐯𝐞𝐫𝐬𝐚𝐫𝐢𝐮 𝐕𝐞𝐭𝐞𝐫𝐚𝐧𝐮 𝐁𝐚 𝐃𝐚𝐥𝐚 𝐕𝐈 𝐗𝐚𝐧𝐚𝐧𝐚: “𝐇𝐚’𝐮 𝐋𝐚 𝐇𝐚𝐥𝐨 𝐁𝐮𝐚𝐭 𝐈𝐝𝐚, 𝐇𝐮𝐬𝐮 𝐁𝐚 𝐉𝐞𝐧𝐞𝐫𝐚𝐥 𝐧𝐨 𝐅𝐮𝐧𝐝𝐚𝐝𝐨𝐫 𝐒𝐢𝐫𝐚, 𝐓𝐚𝐧𝐛𝐚 𝐇𝐚’𝐮 𝐍𝐞’𝐞 𝐒𝐨𝐞𝐡𝐚𝐫𝐭𝐨 𝐊𝐞𝐝𝐮𝐚” \n",
    "\"\"\"\n",
    "\n",
    "plain_text = unidecode(text)\n",
    "plain_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tet'], dtype='<U3')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.predict([plain_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tetun-lid-2afysUYK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2911380a96f00ac3d85554f7d01794a294b1d45d66fe7770d1ab4e2af95b714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
