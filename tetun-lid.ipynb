{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from text_processing.process_data import *\n",
    "from training.train_dev_test import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from text_processing.process_data import ProcessData\n",
    "from text_processing.dea import DataAnalysisExploration\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate classes\n",
    "process_data = ProcessData()\n",
    "dea = DataAnalysisExploration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dili, 11 Outubru 2021 - Gabinete Apoiu Ativida...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Treinamentu ne’e ninia objetivu prinsipál mak ...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iha loron daruak hosi treinamentu ne’e partisi...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Partisipante na’in-56 iha treinamentu ne’e mai...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Komunidade iha suku Bikeli ho Makadade agrades...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence language\n",
       "0  Dili, 11 Outubru 2021 - Gabinete Apoiu Ativida...      tet\n",
       "1  Treinamentu ne’e ninia objetivu prinsipál mak ...      tet\n",
       "2  Iha loron daruak hosi treinamentu ne’e partisi...      tet\n",
       "3  Partisipante na’in-56 iha treinamentu ne’e mai...      tet\n",
       "4  Komunidade iha suku Bikeli ho Makadade agrades...      tet"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths\n",
    "files_folder = Path('lang_files/')\n",
    "files_path = os.path.join(os.getcwd(), files_folder)\n",
    "\n",
    "# initial data\n",
    "lang_data = process_data.compile_all_data(files_path)\n",
    "lang_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "      <th>sentence_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dili  outubru gabinete apoiu atividade kónjuge...</td>\n",
       "      <td>tet</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>treinamentu ne’e ninia objetivu prinsipál mak ...</td>\n",
       "      <td>tet</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iha loron daruak hosi treinamentu ne’e partisi...</td>\n",
       "      <td>tet</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>partisipante na’in iha treinamentu ne’e mai ho...</td>\n",
       "      <td>tet</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>komunidade iha suku bikeli ho makadade agrades...</td>\n",
       "      <td>tet</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence language  sentence_length\n",
       "0  dili  outubru gabinete apoiu atividade kónjuge...      tet              275\n",
       "1  treinamentu ne’e ninia objetivu prinsipál mak ...      tet              232\n",
       "2  iha loron daruak hosi treinamentu ne’e partisi...      tet              288\n",
       "3  partisipante na’in iha treinamentu ne’e mai ho...      tet               82\n",
       "4  komunidade iha suku bikeli ho makadade agrades...      tet              320"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after preprocessed and counted each sentence length\n",
    "clean_data = process_data.clean_data_with_count(files_path)\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is cleaned.\n"
     ]
    }
   ],
   "source": [
    "# confirm that data was cleaned.\n",
    "clean = clean_data[(clean_data['sentence'] =='') & (clean_data['sentence'] ==' ')]\n",
    "try:\n",
    "    assert len(clean) == 0\n",
    "    print(\"The data is cleaned.\")\n",
    "except AssertionError:\n",
    "    print(\"The data is NOT cleaned.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 74258\n",
      "Dev set size: 15913\n",
      "Test set size: 15913\n"
     ]
    }
   ],
   "source": [
    "# Split dataset to train, development(dev)/validation, and test sets\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = train_dev_test_split(clean_data, 0.3, 0.5)\n",
    "\n",
    "# print the sizes of the resulting sets\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Dev set size:\", len(X_dev))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinearSVC()\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.9798\n",
      "\tn_gram 2 --> accuracy:  0.9927\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.9938\n",
      "\tn_gram 2 --> accuracy:  0.9666\n",
      "Model: LogisticRegression(multi_class='ovr')\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.9789\n",
      "\tn_gram 2 --> accuracy:  0.9924\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.9913\n",
      "\tn_gram 2 --> accuracy:  0.9519\n",
      "Model: MultinomialNB()\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.9412\n",
      "\tn_gram 2 --> accuracy:  0.9911\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.9960\n",
      "\tn_gram 2 --> accuracy:  0.9708\n"
     ]
    }
   ],
   "source": [
    "# Compare the models\n",
    "\n",
    "model_lists = [LinearSVC(), LogisticRegression(multi_class='ovr'), MultinomialNB()]\n",
    "analyzers = ['char_wb', 'word']\n",
    "\n",
    "compare_models(model_lists, analyzers, 1, 6, 1, X_train, y_train, X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.99578960598253\n",
      "Confusion Matrix:  [[5047    3    7    0]\n",
      " [  20 4247    3    0]\n",
      " [  16    6 4013    1]\n",
      " [   3    3    5 2539]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "          en       0.99      1.00      1.00      5057\n",
      "          id       1.00      0.99      1.00      4270\n",
      "          pt       1.00      0.99      1.00      4036\n",
      "         tet       1.00      1.00      1.00      2550\n",
      "\n",
      "    accuracy                           1.00     15913\n",
      "   macro avg       1.00      1.00      1.00     15913\n",
      "weighted avg       1.00      1.00      1.00     15913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the best model, train and evaluate it using dev set\n",
    "\n",
    "# Train the model\n",
    "model = train_model(TfidfVectorizer(analyzer='char_wb', ngram_range=(5,5)), MultinomialNB(), X_train, y_train)\n",
    "\n",
    "# Evaluate the model using dev set\n",
    "evaluate_model(model,X_dev, y_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.99578960598253\n",
      "Confusion Matrix:  [[4999    6    4    1]\n",
      " [  22 4253    4    1]\n",
      " [  14    6 4002    1]\n",
      " [   4    0    4 2592]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "          en       0.99      1.00      0.99      5010\n",
      "          id       1.00      0.99      1.00      4280\n",
      "          pt       1.00      0.99      1.00      4023\n",
      "         tet       1.00      1.00      1.00      2600\n",
      "\n",
      "    accuracy                           1.00     15913\n",
      "   macro avg       1.00      1.00      1.00     15913\n",
      "weighted avg       1.00      1.00      1.00     15913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using test set\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 classification: ['tet']\n",
      "Test 2 classification: ['tet']\n"
     ]
    }
   ],
   "source": [
    "test1 = model.predict([\"Organizasaun mundial saúde\"])\n",
    "test2 = model.predict([\"Tribunál rekursu rejeita kandidatura partidu\"])\n",
    "print(f\"Test 1 classification: {test1}\\nTest 2 classification: {test2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deklarasaun Universál Direitus Umanus\n",
      "en 1.7198102347965515e-05\n",
      "id 7.656943219400005e-05\n",
      "pt 0.0001802486040840422\n",
      "tet 0.9997259838613715\n",
      "Indonesia merupakan negara terluas ke-14 sekaligus\n",
      "en 1.3050918693531165e-07\n",
      "id 0.9999995025902451\n",
      "pt 9.055449712082101e-08\n",
      "tet 2.7634607120579004e-07\n",
      "A língua portuguesa, também designada português, é uma língua\n",
      "en 0.00018657712215813118\n",
      "id 2.970099994212303e-05\n",
      "pt 0.998175916996359\n",
      "tet 0.0016078048815381433\n",
      "Deklarasaun ne'e inklui artigu 30 ne'ebé esplika Asembleia Jerál\n",
      "en 2.916091141335691e-08\n",
      "id 4.085156091273892e-08\n",
      "pt 1.4132462405706621e-07\n",
      "tet 0.9999997886629056\n",
      "Can we feed a future population of 10 billion people a healthy?\n",
      "en 0.9999978448630447\n",
      "id 1.321850425292166e-07\n",
      "pt 1.061105899088093e-06\n",
      "tet 9.61846016528177e-07\n"
     ]
    }
   ],
   "source": [
    "input = [\"Deklarasaun Universál Direitus Umanus\", \n",
    "        \"Indonesia merupakan negara terluas ke-14 sekaligus\",\n",
    "        \"A língua portuguesa, também designada português, é uma língua\",\n",
    "        \"Deklarasaun ne'e inklui artigu 30 ne'ebé esplika Asembleia Jerál\",\n",
    "        \"Can we feed a future population of 10 billion people a healthy?\"\n",
    "        ]\n",
    "\n",
    "# Naive Bayes and Logistic Regression\n",
    "pred_probs = model.predict_proba(input)\n",
    "\n",
    "for i, probs in enumerate(pred_probs):\n",
    "    print(input[i])\n",
    "    for j, lang in enumerate(model.classes_):\n",
    "        print(lang, probs[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - LinearSVC\n",
    "pred_result = model.predict(input)\n",
    "pred_probability = model.decision_function(input)\n",
    "for i in range(len(input)):\n",
    "    print(f\"{input[i]} ---> {pred_result[i]} ---> {np.argmax(pred_probability[i])} --> {pred_probability[i]} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model [if required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_best/tet-lid-model_NB_best_ng5chars.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# save the model to a file\n",
    "joblib.dump(model, 'model_best/tet-lid-model_NB_best_ng5chars.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the save model from a file\n",
    "import joblib\n",
    "saved_model = joblib.load('model_best/tet-lid-model_NB_best_ng5chars.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "text = ['Timor-Leste', 'Timor', 'Lei']\n",
    "\n",
    "#plain_text = unidecode(text)\n",
    "#plain_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timor-Leste\n",
      "en 5.971441277656112e-07\n",
      "id 2.524699071902872e-06\n",
      "pt 2.5094790700185165e-06\n",
      "tet 0.9999943686777295\n",
      "Timor\n",
      "en 0.0007093783786956153\n",
      "id 0.006613671115733073\n",
      "pt 0.0006555447235547801\n",
      "tet 0.9920214057820171\n",
      "Lei\n",
      "en 0.02498752277221515\n",
      "id 0.021932794497354923\n",
      "pt 0.4597678041996972\n",
      "tet 0.4933118785307327\n"
     ]
    }
   ],
   "source": [
    "pred_probs = saved_model.predict_proba(text)\n",
    "\n",
    "for i, probs in enumerate(pred_probs):\n",
    "    print(text[i])\n",
    "    for j, lang in enumerate(saved_model.classes_):\n",
    "        print(lang, probs[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tetun-lid-2afysUYK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2911380a96f00ac3d85554f7d01794a294b1d45d66fe7770d1ab4e2af95b714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
