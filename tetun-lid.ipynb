{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_processing.process_data import ProcessData\n",
    "from training.models import TrainModels\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from text_processing.process_data import ProcessData\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classes.\n",
    "process_data = ProcessData()\n",
    "# The model is trained on the chars and words levels, without normalizing the sentence length.\n",
    "dataset = process_data.preprocess_dataset()\n",
    "train_models = TrainModels(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dili outubru gabinete apoiu atividade kónjuge ...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>treinamentu ne’e ninia objetivu prinsipál mak ...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iha loron daruak hosi treinamentu ne’e partisi...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>partisipante na’in-56 iha treinamentu ne’e mai...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>komunidade iha suku bikeli ho makadade agrades...</td>\n",
       "      <td>tet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence language\n",
       "0  dili outubru gabinete apoiu atividade kónjuge ...      tet\n",
       "1  treinamentu ne’e ninia objetivu prinsipál mak ...      tet\n",
       "2  iha loron daruak hosi treinamentu ne’e partisi...      tet\n",
       "3  partisipante na’in-56 iha treinamentu ne’e mai...      tet\n",
       "4  komunidade iha suku bikeli ho makadade agrades...      tet"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review data.\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input data is cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Confirm that data is cleaned.\n",
    "clean = dataset[(dataset[\"sentence\"] == \"\") & (dataset[\"sentence\"] == \" \")]\n",
    "try:\n",
    "    assert len(clean) == 0\n",
    "    print(\"The input data is cleaned.\")\n",
    "except AssertionError:\n",
    "    print(\"The input data is NOT cleaned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dili outubru gabinete apoiu atividade kónjuge prezidente repúblika gaak-pr hala’o treinamentu ‘hakbiit negosiante’ hodi hasa’e negosiante feto sira-nia abilidade durante loron rua iha suku bikeli no loron rua mós iha suku makadade ataúru no hetan partisipante hamutuk na’in-56 \n",
      "treinamentu ne’e ninia objetivu prinsipál mak atu hasa’e no kapasita inan-feton sira kona-ba jestaun finansiamentu nune’e bele jere ho di’ak wainhira hala’o negósiu loro-loron nian hodi bele hatene kapitál ho lukru hira mak sira iha \n",
      "iha loron daruak hosi treinamentu ne’e partisipante sira mós aprende no simu treinamentu kulinária kona-ba oinsá prepara ai-hán lokál ho di’ak nune’e bele utiliza produtu lokál ka materiál lokál nian maibé sabór furak atu bele dada ka atrai liután turizmu lokál no internasionál ba ataúru \n",
      "partisipante na’in-56 iha treinamentu ne’e mai hosi uaruana fatu’u akrema no makadade \n",
      "komunidade iha suku bikeli ho makadade agradese ba espoza pr ne’ebé ho inisativa hakbiit negosiante ki’ik liuhosi programa fó treinamentu nune’e bele hasa’e sira-nia kapasidade atu bele prepara ho di’ak liután sira-nia produtu kulinária iha ataúru hodi simu turista lokál hosi fatin hotu-hotu ne'ebé sei ba vizita ataúru \n",
      "manufahi novembru sekretaria estadu joventude no desportu sejd liuhosi sentru foinsa'e dom boaventura manufahi cefobom iha segunda ne’e apoia fundu ho montante ba grupu foinsa'e kreativu iha munisípiu manufahi hodi dezenvolve atividade pekuária agrikultura no turizmu hodi aumenta rendimentu grupu foinsa’e sira nian \n",
      "pontu fokál sejd livia maria de jesus hateten fundu estimula emprendedorizmu sosiál apoia ba grupu foinsa'e kreativu iha suku nu'udar programa sejd nian liuhosi diresaun nasionál asosiativizmu juventude no mós diresaun kreatividade nasionál juventude no programa refere realiza iha teritóriu timór laran tomak ba suku ho objetivu atu apoia foinsa'e sira nia forsa poténsia kapasidade no kreatividade oinsá mak atu solusiona problema ne'ebé joven sira enfrenta \n",
      "enkuantu jenerál manajer cefobom antoninho doutel sarmento informa fundu ho montante ne'e apoia ba postu-administrativu turiskai same fatuberliu no alas \n",
      "manufahi dezembru eis jornalista rádiu komunidade manufahi domingos rodrigues iha sesta-feira ne’e ezije ba sekretária estadu komunikasaun sosiál sekoms atu halo reestruturasaun ba estrutura no hadi'a jestaun rádiu komunidade manufahi \n",
      "hafoin sosializasaun média munisípiu iha salaun administrasaun munisípiu manufahi eis jornalista ne’e hatete katak ezisténsia rádiu komunidade manufahi hahú eziste iha i governu konstitusionál to’o agora maibé ninia progresu iha programasaun la mudansa to’o agora tán ne’e rekomenda ba sekoms no asosiasaun rádiu komunidade atu hadi'a jestaun rádiu refere hodi divulga informasaun ba públiku ho kualidade \n"
     ]
    }
   ],
   "source": [
    "# Print the top-10 of clean data content.\n",
    "for sentence in dataset['sentence'][:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 79492\n",
      "Dev set size: 17034\n",
      "Test set size: 17035\n"
     ]
    }
   ],
   "source": [
    "# Split dataset to train, development(dev)/validation, and test sets\n",
    "X_train, y_train, X_dev, y_dev, X_test, y_test = train_models.train_dev_test_split(0.3, 0.5)\n",
    "\n",
    "# Print the sizes of the resulting sets\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Dev set size:\", len(X_dev))\n",
    "print(\"Test set size:\", len(X_test))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LinearSVC()\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.98221\n",
      "\tn_gram 2 --> accuracy:  0.99536\n",
      "\tn_gram 3 --> accuracy:  0.99736\n",
      "\tn_gram 4 --> accuracy:  0.99753\n",
      "\tn_gram 5 --> accuracy:  0.99736\n",
      "\tn_gram 6 --> accuracy:  0.99677\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.99530\n",
      "\tn_gram 2 --> accuracy:  0.96889\n",
      "\tn_gram 3 --> accuracy:  0.83967\n",
      "\tn_gram 4 --> accuracy:  0.53669\n",
      "\tn_gram 5 --> accuracy:  0.42075\n",
      "\tn_gram 6 --> accuracy:  0.37308\n",
      "Model: LogisticRegression(multi_class='ovr')\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.98116\n",
      "\tn_gram 2 --> accuracy:  0.99530\n",
      "\tn_gram 3 --> accuracy:  0.99695\n",
      "\tn_gram 4 --> accuracy:  0.99753\n",
      "\tn_gram 5 --> accuracy:  0.99706\n",
      "\tn_gram 6 --> accuracy:  0.99601\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.99296\n",
      "\tn_gram 2 --> accuracy:  0.95221\n",
      "\tn_gram 3 --> accuracy:  0.81073\n",
      "\tn_gram 4 --> accuracy:  0.47482\n",
      "\tn_gram 5 --> accuracy:  0.37918\n",
      "\tn_gram 6 --> accuracy:  0.35006\n",
      "Model: MultinomialNB()\n",
      "Analyzer: char_wb\n",
      "\tn_gram 1 --> accuracy:  0.94523\n",
      "\tn_gram 2 --> accuracy:  0.99184\n",
      "\tn_gram 3 --> accuracy:  0.99671\n",
      "\tn_gram 4 --> accuracy:  0.99765\n",
      "\tn_gram 5 --> accuracy:  0.99806\n",
      "\tn_gram 6 --> accuracy:  0.99789\n",
      "Analyzer: word\n",
      "\tn_gram 1 --> accuracy:  0.99730\n",
      "\tn_gram 2 --> accuracy:  0.97546\n",
      "\tn_gram 3 --> accuracy:  0.78056\n",
      "\tn_gram 4 --> accuracy:  0.50270\n",
      "\tn_gram 5 --> accuracy:  0.39069\n",
      "\tn_gram 6 --> accuracy:  0.35558\n"
     ]
    }
   ],
   "source": [
    "# Compare various models.\n",
    "\n",
    "models_list = [LinearSVC(), LogisticRegression(multi_class=\"ovr\"), MultinomialNB()]\n",
    "analyzers = [\"char_wb\", \"word\"]\n",
    "\n",
    "train_models.compare_models(models_list, analyzers, 1, 6, 1, X_train, y_train, X_dev, y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9980626981331455\n",
      "Confusion Matrix:  [[5205    0    2    0]\n",
      " [  15 4762    1    0]\n",
      " [   4    4 4365    0]\n",
      " [   4    0    3 2669]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "          en    0.99560   0.99962   0.99760      5207\n",
      "          id    0.99916   0.99665   0.99790      4778\n",
      "          pt    0.99863   0.99817   0.99840      4373\n",
      "         tet    1.00000   0.99738   0.99869      2676\n",
      "\n",
      "    accuracy                        0.99806     17034\n",
      "   macro avg    0.99835   0.99796   0.99815     17034\n",
      "weighted avg    0.99807   0.99806   0.99806     17034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the best model, train and evaluate it using the dev set.\n",
    "\n",
    "# Train the model.\n",
    "model = train_models.train_model(\n",
    "    TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(5, 5)), MultinomialNB(), X_train, y_train\n",
    ")\n",
    "\n",
    "# Evaluate the model using dev set.\n",
    "train_models.evaluate_model(model, X_dev, y_dev)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9977105958321104\n",
      "Confusion Matrix:  [[5192    0    3    1]\n",
      " [  11 4685    0    1]\n",
      " [   7    1 4394    4]\n",
      " [   3    0    8 2725]]\n",
      "Classification Report:                precision    recall  f1-score   support\n",
      "\n",
      "          en    0.99597   0.99923   0.99760      5196\n",
      "          id    0.99979   0.99745   0.99861      4697\n",
      "          pt    0.99750   0.99728   0.99739      4406\n",
      "         tet    0.99780   0.99598   0.99689      2736\n",
      "\n",
      "    accuracy                        0.99771     17035\n",
      "   macro avg    0.99777   0.99748   0.99762     17035\n",
      "weighted avg    0.99771   0.99771   0.99771     17035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using test set.\n",
    "train_models.evaluate_model(model, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 classification: ['tet']\n",
      "Test 2 classification: ['tet']\n"
     ]
    }
   ],
   "source": [
    "test1 = model.predict([\"Organizasaun mundial saúde\"])\n",
    "test2 = model.predict([\"Tribunál rekursu rejeita kandidatura partidu\"])\n",
    "print(f\"Test 1 classification: {test1}\\nTest 2 classification: {test2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deklarasaun Universál Direitus Umanus\n",
      "en 2.8512732170291742e-05\n",
      "id 8.949322982940391e-05\n",
      "pt 0.0002959127683900993\n",
      "tet 0.9995860812696081\n",
      "Indonesia merupakan negara terluas ke-14 sekaligus\n",
      "en 2.777319838763683e-07\n",
      "id 0.9999988982394855\n",
      "pt 2.196652093543145e-07\n",
      "tet 6.04363321897563e-07\n",
      "A língua portuguesa, também designada português, é uma língua\n",
      "en 0.00017074857088099135\n",
      "id 3.0375330735971467e-05\n",
      "pt 0.9986205488438126\n",
      "tet 0.0011783272545739549\n",
      "Deklarasaun ne'e inklui artigu 30 ne'ebé esplika Asembleia Jerál\n",
      "en 3.090102033127752e-08\n",
      "id 3.621711208331084e-08\n",
      "pt 1.4671364314537727e-07\n",
      "tet 0.9999997861682262\n",
      "Can we feed a future population of 10 billion people a healthy?\n",
      "en 0.9999982603677747\n",
      "id 1.3167548265092372e-07\n",
      "pt 8.964250178481708e-07\n",
      "tet 7.115317268477991e-07\n"
     ]
    }
   ],
   "source": [
    "input = [\n",
    "    \"Deklarasaun Universál Direitus Umanus\",\n",
    "    \"Indonesia merupakan negara terluas ke-14 sekaligus\",\n",
    "    \"A língua portuguesa, também designada português, é uma língua\",\n",
    "    \"Deklarasaun ne'e inklui artigu 30 ne'ebé esplika Asembleia Jerál\",\n",
    "    \"Can we feed a future population of 10 billion people a healthy?\",\n",
    "]\n",
    "\n",
    "# Naive Bayes and Logistic Regression.\n",
    "pred_probs = model.predict_proba(input)\n",
    "\n",
    "for i, probs in enumerate(pred_probs):\n",
    "    print(input[i])\n",
    "    for j, lang in enumerate(model.classes_):\n",
    "        print(lang, probs[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "# SVM - LinearSVC\n",
    "pred_result = model.predict(input)\n",
    "pred_probability = model.decision_function(input)\n",
    "for i in range(len(input)):\n",
    "    print(\n",
    "        f\"{input[i]} ---> {pred_result[i]} ---> {np.argmax(pred_probability[i])} --> {pred_probability[i]} \"\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model [if required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_best/tet_lid_NB_best_ng5chars_9977.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "#joblib.dump(model, \"model_best/tet_lid_NB_best_ng5chars_9977.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timor-Leste\n",
      "en 5.112054233059201e-07\n",
      "id 1.89281293410119e-06\n",
      "pt 2.696616318632009e-06\n",
      "tet 0.9999948993653245\n",
      "Timor\n",
      "en 0.0005777270313571023\n",
      "id 0.004767330302296208\n",
      "pt 0.0006453106049057784\n",
      "tet 0.9940096320614406\n",
      "Lei\n",
      "en 0.02370068149521924\n",
      "id 0.024037591350435927\n",
      "pt 0.4782344962369875\n",
      "tet 0.4740272309173575\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the model from a file\n",
    "saved_model = joblib.load(\"model_best/tet_lid_NB_best_ng5chars_9977.pkl\")\n",
    "\n",
    "text = [\"Timor-Leste\", \"Timor\", \"Lei\"]\n",
    "\n",
    "pred_probs = saved_model.predict_proba(text)\n",
    "\n",
    "for i, probs in enumerate(pred_probs):\n",
    "    print(text[i])\n",
    "    for j, lang in enumerate(saved_model.classes_):\n",
    "        print(lang, probs[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tetun-lid-2afysUYK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2911380a96f00ac3d85554f7d01794a294b1d45d66fe7770d1ab4e2af95b714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
